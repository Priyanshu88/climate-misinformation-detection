{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7915543,"sourceType":"datasetVersion","datasetId":4650971}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installation\n!pip install pytorch-pretrained-bert pytorch-nlp\n","metadata":{"id":"AR6doFwlNg9k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"442eaeae-c885-42d3-82cd-a1309eab8f4b","execution":{"iopub.status.busy":"2024-03-22T15:27:01.774140Z","iopub.execute_input":"2024-03-22T15:27:01.774755Z","iopub.status.idle":"2024-03-22T15:27:21.522710Z","shell.execute_reply.started":"2024-03-22T15:27:01.774721Z","shell.execute_reply":"2024-03-22T15:27:21.521809Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pytorch-pretrained-bert\n  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl.metadata (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m837.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pytorch-nlp\n  Downloading pytorch_nlp-0.5.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (1.26.4)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (1.26.100)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (4.66.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (2023.12.25)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2024.3.0)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3->pytorch-pretrained-bert)\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-pretrained-bert) (0.6.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (2024.2.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->pytorch-pretrained-bert) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3->pytorch-pretrained-bert) (1.16.0)\nDownloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pytorch-nlp, botocore, pytorch-pretrained-bert\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.34.51\n    Uninstalling botocore-1.34.51:\n      Successfully uninstalled botocore-1.34.51\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.12.1 requires botocore<1.34.52,>=1.34.41, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed botocore-1.29.165 pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Library imports\n\nfrom collections import defaultdict\nimport random\nimport json\nimport pandas as pd\nimport re\nimport tensorflow as tf\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport numpy as np\nimport nltk\nfrom sklearn.metrics import precision_recall_fscore_support","metadata":{"id":"tOcGKRVwODj_","execution":{"iopub.status.busy":"2024-03-22T15:28:54.968200Z","iopub.execute_input":"2024-03-22T15:28:54.968581Z","iopub.status.idle":"2024-03-22T15:29:15.704745Z","shell.execute_reply.started":"2024-03-22T15:28:54.968554Z","shell.execute_reply":"2024-03-22T15:29:15.703059Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-22 15:28:58.302399: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-22 15:28:58.302500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-22 15:28:58.457956: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Paths of json\n\ntrain_json_path = '/kaggle/input/project-files-climate/train.json'\nexternal_json_path = '/kaggle/input/project-files-climate/external.json'\ndev_json_path = '/kaggle/input/project-files-climate/dev.json'\ntest_json_path = '/kaggle/input/project-files-climate/test-unlabelled.json'","metadata":{"id":"G54mGFYcOeqX","execution":{"iopub.status.busy":"2024-03-22T15:32:43.266392Z","iopub.execute_input":"2024-03-22T15:32:43.267039Z","iopub.status.idle":"2024-03-22T15:32:43.271511Z","shell.execute_reply.started":"2024-03-22T15:32:43.267008Z","shell.execute_reply":"2024-03-22T15:32:43.270580Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Defining paths for various datasets and mappings used in a data transformation process, including training, development, and testing data, stored in both TSV and JSON formats\n\nFirst = 2\nLast = 8\n\n# transform data\ntrain_tsv_path = 'train28.tsv'\ndev_tsv_path = 'dev28.tsv'\ntest_tsv_path = 'test28.tsv'\ndev_map_path = 'dev28_map.json'\ntest_map_path = 'test28_map.json'","metadata":{"id":"Am9Xi5bPOekq","execution":{"iopub.status.busy":"2024-03-22T15:32:44.281087Z","iopub.execute_input":"2024-03-22T15:32:44.281447Z","iopub.status.idle":"2024-03-22T15:32:44.286342Z","shell.execute_reply.started":"2024-03-22T15:32:44.281420Z","shell.execute_reply":"2024-03-22T15:32:44.285423Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# The provided code performs data preprocessing tasks, including converting JSON-formatted text data into TSV format and splitting articles into paragraphs, preparing the data for further analysis or modeling\n\ndef JsonToTsv(target_file, *files):\n    texts = []\n    labels = []\n    for file in files:\n\n        df = pd.read_json(file)\n        text = list(df.loc['text'].values)\n        try:\n            label = list(df.loc['label'].values)\n        except:\n            label = [0 for _ in range(len(text))]\n        texts.extend(text)\n        labels.extend(label)\n\n    df = pd.DataFrame({\n        'sentence':texts,\n        'label':labels\n    })\n\n    df.to_csv(target_file, sep='\\t', index=False, header=True)\n\nimport re\ndef tsvArticleToTsvPara(source, target):\n    df = pd.read_csv(source, sep='\\t')\n    all_paras = []\n    all_labels = []\n\n    d = defaultdict(int)\n    para_id = 0\n\n    for i, (texts, label) in enumerate(df.values):\n\n        paras = re.split('\\n', texts)\n        paras = [p for p in paras if len(p)>10]\n        para_index = [i for i in range(len(paras))]\n\n\n        former = para_index[:2]\n        later = para_index[-8:]\n        index = former + later\n        for _ in range(len(index)):\n          d[para_id] = i\n          para_id+=1\n        all_paras.extend([paras[i] for i in index])\n        all_labels.extend([label for _ in index])\n\n\n    df = pd.DataFrame({\n        'sentence':all_paras,\n        'label':all_labels\n    })\n    df.to_csv(target, sep='\\t', index=False, header=True)\n    return d\n\n\ndef generate_all_para(name):\n    for n in ['train', 'dev', 'test']:\n        if n == 'train':\n            ofs = [train_json_path, external_json_path]\n        elif n == 'test':\n            ofs = [test_json_path,]\n        else:\n            ofs = [dev_json_path,]\n        JsonToTsv(n+'.tsv', *ofs)\n        d = tsvArticleToTsvPara(n+'.tsv', n+name+'.tsv')\n        with open(n+name+'_map.json', 'w') as f:\n            json.dump(d, f)","metadata":{"id":"QVnEkbVZOegd","execution":{"iopub.status.busy":"2024-03-22T15:32:44.504767Z","iopub.execute_input":"2024-03-22T15:32:44.505441Z","iopub.status.idle":"2024-03-22T15:32:44.519927Z","shell.execute_reply.started":"2024-03-22T15:32:44.505415Z","shell.execute_reply":"2024-03-22T15:32:44.519018Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Executes the data transformation process, generating paragraph-level TSV files and corresponding mapping JSON files for the specified range of datasets, typically denoted by the concatenation of the \"First\" and \"Last\" variables\ngenerate_all_para(str(First)+str(Last))","metadata":{"id":"xiG4juBAOedz","execution":{"iopub.status.busy":"2024-03-22T15:32:44.872681Z","iopub.execute_input":"2024-03-22T15:32:44.873276Z","iopub.status.idle":"2024-03-22T15:32:47.984865Z","shell.execute_reply.started":"2024-03-22T15:32:44.873251Z","shell.execute_reply":"2024-03-22T15:32:47.983928Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# GPU Test\n\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n    raise SystemError('GPU device not found')\nprint('Found GPU: {}'.format(device_name))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cw_-pco7OebK","outputId":"2530179f-2402-4d36-cf2f-488897d427d4","execution":{"iopub.status.busy":"2024-03-22T15:34:13.104807Z","iopub.execute_input":"2024-03-22T15:34:13.105160Z","iopub.status.idle":"2024-03-22T15:34:13.449938Z","shell.execute_reply.started":"2024-03-22T15:34:13.105132Z","shell.execute_reply":"2024-03-22T15:34:13.448881Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Found GPU: /device:GPU:0\n","output_type":"stream"}]},{"cell_type":"code","source":"# GPU Name\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"D4mRdh0JOeYQ","outputId":"33725725-1426-4bf3-d9d6-7ca6b9f1b3e2","execution":{"iopub.status.busy":"2024-03-22T15:34:23.827525Z","iopub.execute_input":"2024-03-22T15:34:23.827891Z","iopub.status.idle":"2024-03-22T15:34:23.865192Z","shell.execute_reply.started":"2024-03-22T15:34:23.827860Z","shell.execute_reply":"2024-03-22T15:34:23.864364Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'Tesla T4'"},"metadata":{}}]},{"cell_type":"code","source":"# Reading the training data from a TSV file specified by `train_tsv_path` into a pandas DataFrame (`trn_df`). It expects the TSV file to have columns named 'sentence' and 'label'. Finally, it retrieves the shape of the DataFrame, indicating the number of rows and columns in the training data\n\ntrn_df = pd.read_csv(train_tsv_path, delimiter='\\t', header=0, names=['sentence', 'label'])\ntrn_df.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DE4ddpclOeRZ","outputId":"0070ff3b-97cf-4c2a-9bea-41e9c304419c","execution":{"iopub.status.busy":"2024-03-22T15:34:25.475107Z","iopub.execute_input":"2024-03-22T15:34:25.475458Z","iopub.status.idle":"2024-03-22T15:34:25.551789Z","shell.execute_reply.started":"2024-03-22T15:34:25.475430Z","shell.execute_reply":"2024-03-22T15:34:25.550928Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(22767, 2)"},"metadata":{}}]},{"cell_type":"code","source":"# Defining a function to segment input texts into sentences using NLTK's sentence tokenizer and formats them for BERT model input, while downloading the necessary NLTK data for tokenization\n\nnltk.download('punkt')\ndef get_segmented_data(texts):\n    segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n    segmented_texts = []\n    for text in texts:\n        text = text.split('\\n')\n        segmented_text = '[CLS] '\n        for para in text:.\n            for sentence in segmenter.tokenize(para):\n                segmented_text += sentence + ' [SEP] '\n        segmented_text = segmented_text[:-1]\n        segmented_texts.append(segmented_text)\n    return segmented_texts","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6cPKp1bPlw8","outputId":"46c8ad6c-94a9-4800-b48d-9d011683b5f8","execution":{"iopub.status.busy":"2024-03-22T15:34:26.663109Z","iopub.execute_input":"2024-03-22T15:34:26.663460Z","iopub.status.idle":"2024-03-22T15:34:26.844376Z","shell.execute_reply.started":"2024-03-22T15:34:26.663431Z","shell.execute_reply":"2024-03-22T15:34:26.843409Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Segmenting sentences from the training data into BERT-compatible format using the defined function `get_segmented_data()` and extracts corresponding labels for classification from the training DataFrame\nsentences = get_segmented_data(trn_df.sentence.values)\nlabels = [int(i) for i in trn_df.label.values]","metadata":{"id":"uelE4fIOPluo","execution":{"iopub.status.busy":"2024-03-22T15:34:27.722813Z","iopub.execute_input":"2024-03-22T15:34:27.723169Z","iopub.status.idle":"2024-03-22T15:34:29.498407Z","shell.execute_reply.started":"2024-03-22T15:34:27.723142Z","shell.execute_reply":"2024-03-22T15:34:29.497421Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Initializing a BERT tokenizer from the 'bert-base-uncased' pre-trained model and tokenizes the segmented texts obtained from the training data. It then prints the tokenized form of the first sentence\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1t36lXRPlr8","outputId":"18fd577e-7348-4319-9260-c07403039094","execution":{"iopub.status.busy":"2024-03-22T15:34:29.500008Z","iopub.execute_input":"2024-03-22T15:34:29.500296Z","iopub.status.idle":"2024-03-22T15:34:54.654150Z","shell.execute_reply.started":"2024-03-22T15:34:29.500273Z","shell.execute_reply":"2024-03-22T15:34:54.653185Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 231508/231508 [00:00<00:00, 2018372.79B/s]\n","output_type":"stream"},{"name":"stdout","text":"Tokenize the first sentence:\n['[CLS]', 'why', 'houston', 'flooding', 'isn', '‘', 't', 'a', 'sign', 'of', 'climate', 'change', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Converting the tokenized texts into token IDs using the BERT tokenizer, padding the sequences to a maximum length of 128 tokens, and creating attention masks to indicate the presence of actual tokens versus padded tokens for each sequence\n\nMAX_LEN = 128\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n# Pad input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12UFIu5HPlpU","outputId":"726cf560-d492-4359-b146-c5860c86ec20","execution":{"iopub.status.busy":"2024-03-22T15:37:16.663125Z","iopub.execute_input":"2024-03-22T15:37:16.663987Z","iopub.status.idle":"2024-03-22T15:37:19.097424Z","shell.execute_reply.started":"2024-03-22T15:37:16.663953Z","shell.execute_reply":"2024-03-22T15:37:19.096668Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Spliting the input token IDs and attention masks into training and validation sets using a 90-10 split ratio while also splitting the corresponding labels accordingly, with a fixed random state for reproducibility\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state=2018, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)","metadata":{"id":"AQpw-DNbPlmP","execution":{"iopub.status.busy":"2024-03-22T15:37:24.570242Z","iopub.execute_input":"2024-03-22T15:37:24.571094Z","iopub.status.idle":"2024-03-22T15:37:24.618117Z","shell.execute_reply.started":"2024-03-22T15:37:24.571059Z","shell.execute_reply":"2024-03-22T15:37:24.617022Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Converting the training and validation input sequences, labels, and attention masks into PyTorch tensors for further processing with a neural network model\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n","metadata":{"id":"XHiSvA9OQAuD","execution":{"iopub.status.busy":"2024-03-22T15:37:54.904780Z","iopub.execute_input":"2024-03-22T15:37:54.905518Z","iopub.status.idle":"2024-03-22T15:37:54.920285Z","shell.execute_reply.started":"2024-03-22T15:37:54.905488Z","shell.execute_reply":"2024-03-22T15:37:54.919323Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2960893429.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  train_inputs = torch.tensor(train_inputs)\n/tmp/ipykernel_34/2960893429.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  validation_inputs = torch.tensor(validation_inputs)\n/tmp/ipykernel_34/2960893429.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  train_labels = torch.tensor(train_labels)\n/tmp/ipykernel_34/2960893429.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  validation_labels = torch.tensor(validation_labels)\n/tmp/ipykernel_34/2960893429.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  train_masks = torch.tensor(train_masks)\n/tmp/ipykernel_34/2960893429.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  validation_masks = torch.tensor(validation_masks)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating a PyTorch DataLoader for the training data, which is iterating over batches of input sequences, attention masks, and labels, using random sampling from the training dataset with a specified batch size of 32\n\nbatch_size = 32\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)","metadata":{"id":"ZKkP_iwGPljA","execution":{"iopub.status.busy":"2024-03-22T15:37:59.054884Z","iopub.execute_input":"2024-03-22T15:37:59.055476Z","iopub.status.idle":"2024-03-22T15:37:59.064230Z","shell.execute_reply.started":"2024-03-22T15:37:59.055446Z","shell.execute_reply":"2024-03-22T15:37:59.063372Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Creating a PyTorch DataLoader for the validation data, iterating over batches of input sequences, attention masks, and labels in sequential order, using a specified batch size of 32\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"id":"EOXgUgyJP67N","execution":{"iopub.status.busy":"2024-03-22T15:38:00.878842Z","iopub.execute_input":"2024-03-22T15:38:00.879726Z","iopub.status.idle":"2024-03-22T15:38:00.884504Z","shell.execute_reply.started":"2024-03-22T15:38:00.879693Z","shell.execute_reply":"2024-03-22T15:38:00.883533Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Initializing a BERT model for sequence classification with 2 labels (binary classification) based on the 'bert-base-uncased' pre-trained model, and moves the model parameters to the GPU for faster computation\n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel.cuda()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WxA8ehxQH7H","outputId":"7d848582-8813-484b-bb52-df52d0256e28","execution":{"iopub.status.busy":"2024-03-22T15:38:01.978517Z","iopub.execute_input":"2024-03-22T15:38:01.978889Z","iopub.status.idle":"2024-03-22T15:38:19.618426Z","shell.execute_reply.started":"2024-03-22T15:38:01.978862Z","shell.execute_reply":"2024-03-22T15:38:19.617511Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"100%|██████████| 407873900/407873900 [00:09<00:00, 41075219.85B/s]\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Setting up the parameters and their respective decay rates for the optimizer. Parameters with names containing 'bias', 'gamma', or 'beta' are assigned a weight decay rate of 0.01, while others are assigned a weight decay rate of 0.0\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]","metadata":{"id":"6dx5TiC_QNo1","execution":{"iopub.status.busy":"2024-03-22T15:38:23.094066Z","iopub.execute_input":"2024-03-22T15:38:23.094766Z","iopub.status.idle":"2024-03-22T15:38:23.101704Z","shell.execute_reply.started":"2024-03-22T15:38:23.094731Z","shell.execute_reply":"2024-03-22T15:38:23.100803Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Initializing the optimizer using the BertAdam optimizer with the specified grouped parameters, learning rate of 2e-5, and warmup proportion of 0.1\noptimizer = BertAdam(optimizer_grouped_parameters, lr=2e-5, warmup=.1)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dx4G7wWhQNlR","outputId":"888c1a44-63cc-4f3e-d330-f74ce83fc3be","execution":{"iopub.status.busy":"2024-03-22T15:38:24.549960Z","iopub.execute_input":"2024-03-22T15:38:24.550792Z","iopub.status.idle":"2024-03-22T15:38:25.944620Z","shell.execute_reply.started":"2024-03-22T15:38:24.550763Z","shell.execute_reply":"2024-03-22T15:38:25.943799Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Calculating the accuracy of predictions compared to the actual labels by flattening the predicted values and labels, and then computing the proportion of correctly predicted labels\n\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"id":"lpV6TWXbQNif","execution":{"iopub.status.busy":"2024-03-22T15:39:25.326452Z","iopub.execute_input":"2024-03-22T15:39:25.327354Z","iopub.status.idle":"2024-03-22T15:39:25.332269Z","shell.execute_reply.started":"2024-03-22T15:39:25.327323Z","shell.execute_reply":"2024-03-22T15:39:25.331333Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Training the BERT model for the specified number of epochs, iterating through each epoch and performing training and validation steps. During training, computing the loss, performs backpropagation, and updating the model parameters. During validation, evaluating the model's accuracy on the validation dataset\n\nt = []\ntrain_loss_set = []\nepochs = 4\n\nfor _ in trange(epochs, desc=\"Epoch\"):\n\n    # Training\n\n    model.train()\n\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n\n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n        # Forward pass\n        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        train_loss_set.append(loss.item())\n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n\n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n\n\n    # Validation\n\n    model.eval()\n\n    # Tracking variables\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions\n            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n\n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n\n    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gx-GRzQrQW6A","outputId":"ff93b476-89ee-42fe-9567-80cd00ff03a5","execution":{"iopub.status.busy":"2024-03-22T15:39:26.482192Z","iopub.execute_input":"2024-03-22T15:39:26.482566Z","iopub.status.idle":"2024-03-22T16:14:06.015132Z","shell.execute_reply.started":"2024-03-22T15:39:26.482519Z","shell.execute_reply":"2024-03-22T16:14:06.014201Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Epoch:   0%|          | 0/4 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n\tadd_(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\n\tadd_(Tensor other, *, Number alpha) (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/python_arg_parser.cpp:1519.)\n  next_m.mul_(beta1).add_(1 - beta1, grad)\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.23328859803910468\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  25%|██▌       | 1/4 [08:39<25:58, 519.59s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.94140625\nTrain loss: 0.07159126466852082\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  50%|█████     | 2/4 [17:19<17:19, 519.66s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.9557291666666666\nTrain loss: 0.024666933524020978\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  75%|███████▌  | 3/4 [25:59<08:39, 519.79s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.94140625\nTrain loss: 0.0102150945336652\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|██████████| 4/4 [34:39<00:00, 519.88s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.95703125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# predict(tsv_file_path, model) reading a TSV file containing sentences and their labels, tokenizes the sentences, prepares them for prediction, and then making predictions using the provided BERT model and returning the predicted labels\n\ndef predict(tsv_file_path, model):\n    df = pd.read_csv(tsv_file_path, delimiter='\\t', header=0, names=['sentence', 'label'])\n\n    sentences = df.sentence.values\n    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n    labels = [int(i) for i in df.label.values]\n\n    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n\n    MAX_LEN = 128\n    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n    attention_masks = []\n\n    for seq in input_ids:\n        seq_mask = [float(i>0) for i in seq]\n        attention_masks.append(seq_mask)\n\n    prediction_inputs = torch.tensor(input_ids)\n    prediction_masks = torch.tensor(attention_masks)\n    prediction_labels = torch.tensor(labels)\n\n    batch_size = 32\n\n    prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n    prediction_sampler = SequentialSampler(prediction_data)\n    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n\n\n    model.eval()\n    predictions , true_labels = [], []\n\n    # Predict\n    for batch in prediction_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        with torch.no_grad():\n            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        predictions.append(logits)\n        true_labels.append(label_ids)\n\n    flat_predictions = [item for sublist in predictions for item in sublist]\n    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n    return flat_predictions","metadata":{"id":"f29e2fRwQW2Y","execution":{"iopub.status.busy":"2024-03-22T16:14:06.016628Z","iopub.execute_input":"2024-03-22T16:14:06.016922Z","iopub.status.idle":"2024-03-22T16:14:06.028386Z","shell.execute_reply.started":"2024-03-22T16:14:06.016897Z","shell.execute_reply":"2024-03-22T16:14:06.027555Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Using provided BERT model to make predictions on the development dataset (`dev_tsv_path`), returning the predicted labels\n\ndev_flat_predictions = predict(dev_tsv_path, model)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9j12nCiRQWzR","outputId":"7fd9fc1f-7381-45fd-a2e9-e511a15613b0","execution":{"iopub.status.busy":"2024-03-22T16:17:56.788701Z","iopub.execute_input":"2024-03-22T16:17:56.789128Z","iopub.status.idle":"2024-03-22T16:18:05.480362Z","shell.execute_reply.started":"2024-03-22T16:17:56.789092Z","shell.execute_reply":"2024-03-22T16:18:05.479600Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# `read_data_from_json(file_path)` readin labels from a JSON file located at `file_path` and returning them as a list of integers, then reading the labels for the development dataset from a JSON file and loads the corresponding mapping JSON file\n\ndef read_data_from_json(file_path):\n    df = pd.read_json(file_path)\n    labels = [int(i) for i in df.loc['label'].values]\n    return labels\ndev_labels = read_data_from_json(dev_json_path)\n\nwith open(dev_map_path,'r') as f:\n    map = json.load(f)","metadata":{"id":"avD_4vnvQWwA","execution":{"iopub.status.busy":"2024-03-22T16:18:07.880321Z","iopub.execute_input":"2024-03-22T16:18:07.881162Z","iopub.status.idle":"2024-03-22T16:18:07.950613Z","shell.execute_reply.started":"2024-03-22T16:18:07.881132Z","shell.execute_reply":"2024-03-22T16:18:07.949845Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Adjusts the predictions made on the development dataset (`dev_flat_predictions`) based on a mapping (`map`) and a threshold rule. It counts the number of positive predictions for each original instance, updates them based on the threshold (if more than 4), and then prints the count of positive predictions\n\nnew_predictions = [0] * len(dev_labels)\nfor i, result in enumerate(dev_flat_predictions):\n    if result == 1:\n        new_predictions[map[str(i)]] += 1\n\nfor i in range(len(new_predictions)):\n    if new_predictions[i] >4:\n        new_predictions[i] = 1\n    else:\n        new_predictions[i]=0\n\nprint(len([i for i in new_predictions if i==1]))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLgrNB8RQWs4","outputId":"5b835a8f-c72e-44f1-9166-7772df9e46dd","execution":{"iopub.status.busy":"2024-03-22T16:18:08.385424Z","iopub.execute_input":"2024-03-22T16:18:08.386302Z","iopub.status.idle":"2024-03-22T16:18:08.393585Z","shell.execute_reply.started":"2024-03-22T16:18:08.386269Z","shell.execute_reply":"2024-03-22T16:18:08.392582Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"55\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculating the precision, recall, and F1-score metrics using the `precision_recall_fscore_support` function from scikit-learn, comparing the predicted labels (`new_predictions`) with the actual labels (`dev_labels`) for the positive class (pos_label=1). Then, it prints out the precision, recall, and F1-score values\n\np, r, f, _ = precision_recall_fscore_support(dev_labels, new_predictions, pos_label=1, average=\"binary\")\nprint('precision:',p)\nprint('recall:',r)\nprint('f_score:',f)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tSO_v5oQWqG","outputId":"0bdba230-1791-4977-a47d-3a1dca97e6b1","execution":{"iopub.status.busy":"2024-03-22T16:18:10.988284Z","iopub.execute_input":"2024-03-22T16:18:10.988772Z","iopub.status.idle":"2024-03-22T16:18:11.001065Z","shell.execute_reply.started":"2024-03-22T16:18:10.988741Z","shell.execute_reply":"2024-03-22T16:18:11.000224Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"precision: 0.8363636363636363\nrecall: 0.92\nf_score: 0.8761904761904761\n","output_type":"stream"}]},{"cell_type":"code","source":"# Using the provided BERT model to make predictions on the test dataset (`test_tsv_path`), returning the predicted labels\n\ntest_flat_predictions = predict(test_tsv_path, model)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mteHKt2RaVNk","outputId":"60268ac6-b0fd-451c-e9d8-1b577c145919","execution":{"iopub.status.busy":"2024-03-22T16:18:13.487943Z","iopub.execute_input":"2024-03-22T16:18:13.488750Z","iopub.status.idle":"2024-03-22T16:20:12.466276Z","shell.execute_reply.started":"2024-03-22T16:18:13.488718Z","shell.execute_reply":"2024-03-22T16:20:12.465283Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Reading the mapping JSON file for the test dataset from the specified path and loads it into a dictionary named `test_map`\n\nwith open(test_map_path,'r') as f:\n    test_map = json.load(f)\n","metadata":{"id":"xBh18ARNaVJ_","execution":{"iopub.status.busy":"2024-03-22T16:20:28.524368Z","iopub.execute_input":"2024-03-22T16:20:28.525006Z","iopub.status.idle":"2024-03-22T16:20:28.534449Z","shell.execute_reply.started":"2024-03-22T16:20:28.524975Z","shell.execute_reply":"2024-03-22T16:20:28.533589Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Adjusting the predictions made on the test dataset (`test_flat_predictions`) based on a mapping (`test_map`) and a threshold rule. Counting the number of positive predictions for each original instance, updating them based on the threshold (if more than 4), and then printing the count of positive predictions\n\nnew_predictions = [0] * (max(test_map.values())+1)\nfor i, result in enumerate(test_flat_predictions):\n    if result == 1:\n        new_predictions[test_map[str(i)]] += 1\n\nfor i in range(len(new_predictions)):\n    if new_predictions[i] >4:\n        new_predictions[i] = 1\n    else: new_predictions[i]=0\n\nprint(len([i for i in new_predictions if i>0]))","metadata":{"id":"eLSkCpe7aVGh","execution":{"iopub.status.busy":"2024-03-22T16:20:29.869945Z","iopub.execute_input":"2024-03-22T16:20:29.870268Z","iopub.status.idle":"2024-03-22T16:20:29.883609Z","shell.execute_reply.started":"2024-03-22T16:20:29.870246Z","shell.execute_reply":"2024-03-22T16:20:29.882597Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"148\n","output_type":"stream"}]},{"cell_type":"code","source":"# Constructing a dictionary `test_dict` where each entry corresponds to a test instance. The predicted label for each instance is set to 1 if the prediction probability is greater than or equal to 0.5, otherwise, it's set to 0\n\ntest_dict = {}\nfor i in range(len(new_predictions)):\n    label = 1 if new_predictions[i]>= 0.5 else 0\n    test_dict['test-'+str(i)] = {'label':label}","metadata":{"id":"qPpxiH3GaVDz","execution":{"iopub.status.busy":"2024-03-22T16:20:31.292200Z","iopub.execute_input":"2024-03-22T16:20:31.292881Z","iopub.status.idle":"2024-03-22T16:20:31.298967Z","shell.execute_reply.started":"2024-03-22T16:20:31.292850Z","shell.execute_reply":"2024-03-22T16:20:31.297971Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Defining a function `save_to_zip(pred_dict, zip_name)` that saves a dictionary `pred_dict` to a JSON file named 'test-output.json', and then creating a zip file with the given name (`zip_name`) containing the JSON file. The name of the zip file is based on the current date and time, truncated to the hour. Additionally, returning the current date and time truncated to the hour\n\nimport zipfile\nimport datetime\ndef save_to_zip(pred_dict, zip_name):\n    with open('test-output.json', 'w') as f:\n        json.dump(pred_dict, f)\n    z = zipfile.ZipFile(zip_name+'.zip', 'w', zipfile.ZIP_STORED)\n    z.write('test-output.json', 'test-output.json')\n    z.close()\nsave_to_zip(test_dict, str(datetime.datetime.now())[:13])\nstr(datetime.datetime.now())[:13]","metadata":{"id":"aVFKpsEuaUx9","execution":{"iopub.status.busy":"2024-03-22T16:20:33.631946Z","iopub.execute_input":"2024-03-22T16:20:33.632696Z","iopub.status.idle":"2024-03-22T16:20:33.649953Z","shell.execute_reply.started":"2024-03-22T16:20:33.632665Z","shell.execute_reply":"2024-03-22T16:20:33.649113Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'2024-03-22 16'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}